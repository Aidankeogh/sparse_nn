{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import write_dot, graphviz_layout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from coolname import generate_slug as name\n",
    "import yaml\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_ins = []\n",
    "one_ins = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(self, grad_input, grad_output):\n",
    "    temp = list(grad_input)\n",
    "    #Temp[2] is the gradients to the weights, zero them out so they cant change. \n",
    "    temp[2] *= torch.transpose(self.mask,0, 1)\n",
    "    return tuple(temp)\n",
    "\n",
    "class SparseNet(nn.Module):\n",
    "    def __init__(self, weights, biases, masks):\n",
    "        super(SparseNet, self).__init__()\n",
    "        self.act_log = None\n",
    "        self.err_log = None\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.masks = masks\n",
    "        self.out = None\n",
    "        for w, b, m in zip(weights,biases,masks):\n",
    "            temp = nn.Linear(w.shape[1],w.shape[0])\n",
    "            temp.weight.data = torch.from_numpy(w.astype(np.float32))\n",
    "            temp.bias.data = torch.from_numpy(b.astype(np.float32))\n",
    "            temp.mask = torch.from_numpy(m.astype(np.float32))\n",
    "            temp.register_backward_hook(zero_grad)\n",
    "            self.layers.append(temp)\n",
    "\n",
    "    def forward(self, x, log=False):\n",
    "        self.log = log\n",
    "        \n",
    "        for l in self.layers[:-1]:\n",
    "            y = l(x)\n",
    "            x = torch.cat((x,y),1)\n",
    "            x = F.relu(x)\n",
    "        self.out = self.layers[-1](x)\n",
    "        \n",
    "        if(log):\n",
    "            self.out.retain_grad()\n",
    "            if self.act_log is None:\n",
    "                self.act_log = x.detach()\n",
    "            else:\n",
    "                self.act_log = torch.cat((self.act_log,x.detach()), dim=0)\n",
    "            \n",
    "        return self.out\n",
    "    \n",
    "    def clear(self):\n",
    "        self.act_log = None\n",
    "        self.err_log = None\n",
    "\n",
    "    def dumpweights(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for l in self.layers:\n",
    "            weights.append(l.weight.data.numpy().astype(np.float64))\n",
    "            biases.append(l.bias.data.numpy().astype(np.float64))\n",
    "        return weights, biases\n",
    "\n",
    "class graph_neural_network():\n",
    "    def __init__(self,n_in,n_out,h_edges=3,in_names = None, out_names = None):\n",
    "        self.G = nx.DiGraph()\n",
    "        self.h_edges = h_edges\n",
    "        self.inputs = in_names\n",
    "        self.outputs = out_names\n",
    "        if in_names is None:\n",
    "            self.inputs = [\"(in)\"+name() for i in range(n_in)]\n",
    "        if out_names is None:\n",
    "            self.outputs = [\"(out)\"+name() for i in range(n_out)]\n",
    "        self.flat = None\n",
    "        for i in self.inputs:\n",
    "            self.G.add_node(i) \n",
    "        for o in self.outputs:\n",
    "            self.G.add_node(o)\n",
    "            self.G.node[o]['bias'] = np.random.normal(0,1) \n",
    "        self.hidden = []\n",
    "    \n",
    "    def add_hidden(self,incoming,outgoing):\n",
    "        h = name()\n",
    "        #print(incoming,outgoing)\n",
    "        edges = []\n",
    "        for i in incoming:\n",
    "            edges.append((i,h,np.random.normal(0,1.0/self.h_edges)))\n",
    "        \n",
    "        approx_xavier = len(self.outputs) * 1.0/ (1+len(self.hidden))\n",
    "        \n",
    "        for o in outgoing:\n",
    "            edges.append((h,o,np.random.normal(0,approx_xavier)))\n",
    "            \n",
    "        self.G.add_weighted_edges_from(edges)\n",
    "        self.G.node[h]['bias'] = random.uniform(-.1,.1)\n",
    "        self.hidden.append(h)\n",
    "    \n",
    "    def add_random_hidden(self):\n",
    "        incoming = [random.choice(self.inputs+self.hidden) for i in range(self.h_edges)]\n",
    "        outgoing = [random.choice(self.outputs)]\n",
    "        self.add_hidden(incoming,outgoing)\n",
    "    \n",
    "    def get_layers(self):\n",
    "        G = self.G\n",
    "        G2 = nx.topological_sort(G)\n",
    "        max_layer = 0\n",
    "        for n in G2:\n",
    "            if n not in self.outputs:\n",
    "                G.node[n]['layer'] = max([G.node[k[0]]['layer'] for k in G.in_edges(n)] + [-1]) + 1\n",
    "                max_layer = max(max_layer,G.node[n]['layer'])\n",
    "        for n in self.outputs:\n",
    "            G.node[n]['layer'] = max_layer + 1\n",
    "\n",
    "        layers = [[] for _ in range(max_layer + 2)]\n",
    "\n",
    "        for n in G:\n",
    "            layers[G.node[n]['layer']].append(n)\n",
    "\n",
    "        self.flat = []\n",
    "        flat_idx = 0\n",
    "        for i, layer in enumerate(layers):\n",
    "            for j, n in enumerate(layer):\n",
    "                G.node[n]['idx'] = j\n",
    "                G.node[n]['flat_idx'] = flat_idx\n",
    "                self.flat.append(n)\n",
    "                flat_idx += 1\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def get_output_idxes(self):\n",
    "        outputs = self.outputs\n",
    "        G = self.G\n",
    "        out_idxes = []\n",
    "        for n in G:\n",
    "            if(n in outputs):\n",
    "                out_idxes.append(G.node[n]['flat_idx'])\n",
    "        return out_idxes\n",
    "    \n",
    "    def get_weights(self):\n",
    "        G = self.G\n",
    "        outputs = self.outputs\n",
    "        \n",
    "        layers = self.get_layers()\n",
    "        mask = []    \n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        n_nodes = 0\n",
    "        for i in range(len(layers) - 1):\n",
    "            n_nodes += len(layers[i])\n",
    "            mask.append(np.zeros((len(layers[i+1]),n_nodes)))\n",
    "            biases.append(np.zeros((len(layers[i+1]))))\n",
    "            weights.append(np.zeros((len(layers[i+1]),n_nodes)))\n",
    "\n",
    "            for j, node1 in enumerate(layers[i+1]):\n",
    "                biases[i][j] = G.node[node1]['bias']\n",
    "                for node0, _ in G.in_edges(node1): \n",
    "                    u = G.node[node0]['flat_idx']\n",
    "                    v = G.node[node1]['idx']\n",
    "                    mask[i][v,u] = 1\n",
    "                    weights[i][v,u] = G[node0][node1]['weight']\n",
    "\n",
    "        return weights, biases, mask\n",
    "    \n",
    "    def set_weights(self,weights,biases):\n",
    "        layers = self.get_layers()\n",
    "        for i in range(len(layers) - 1):\n",
    "            for j, node1 in enumerate(layers[i+1]):\n",
    "                self.G.node[node1]['bias'] = biases[i][j]\n",
    "                for node0, _ in self.G.in_edges(node1): \n",
    "                    u = self.G.node[node0]['flat_idx']\n",
    "                    v = self.G.node[node1]['idx']\n",
    "                    self.G[node0][node1]['weight'] = weights[i][v,u]\n",
    "                    \n",
    "    def create_nn(self):\n",
    "        w, b, m = self.get_weights()\n",
    "        self.nn = SparseNet(w,b,m)\n",
    "    \n",
    "    def update_graph(self):\n",
    "        weights = [l.weight.data.numpy() for l in self.nn.layers]\n",
    "        biases = [l.bias.data.numpy() for l in self.nn.layers]\n",
    "        self.set_weights(weights,biases)\n",
    "    \n",
    "    def get_err_act_vectors(self,X,y):\n",
    "        gnn.nn.clear()\n",
    "        for j in range(len(X)):\n",
    "            out = gnn.nn(X[j],log=True) \n",
    "            out = F.softmax(out,dim=1)\n",
    "            loss = criterion(out,y[j])\n",
    "            loss.backward()\n",
    "\n",
    "            if gnn.nn.err_log is None:\n",
    "                gnn.nn.err_log = gnn.nn.out.grad\n",
    "            else:\n",
    "                gnn.nn.err_log = torch.cat((gnn.nn.err_log,gnn.nn.out.grad), dim=0)\n",
    "                \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        return gnn.nn.act_log, gnn.nn.err_log\n",
    "    \n",
    "    def add_cossim_hidden(self,X,y,out=None):\n",
    "        \n",
    "        act, err = self.get_err_act_vectors(X,y)\n",
    "        if out is None:\n",
    "            out = random.randint(0,len(self.outputs)-1)\n",
    "        err = err[:,out:out+1]\n",
    "        \n",
    "        #print(act.size(),err.size())\n",
    "        act = F.normalize(act)\n",
    "        err = F.normalize(err)\n",
    "        \n",
    "        incoming = []\n",
    "        toprint = []\n",
    "        cossim = F.cosine_similarity(err,act,dim=0)\n",
    "        _, choices = torch.topk(cossim.abs(),self.h_edges, dim=0)\n",
    "        for choice in choices:\n",
    "            incoming.append(self.flat[choice])\n",
    "            toprint.append((\"%.2f(\" % cossim[choice]) + self.flat[choice]+\")\")\n",
    "            if out == 0:\n",
    "                zero_ins.append(self.flat[choice].split(\",\"))\n",
    "            if out == 1:\n",
    "                one_ins.append(self.flat[choice].split(\",\"))\n",
    "            \n",
    "        outgoing = [self.outputs[out]]\n",
    "        print(toprint,\"->\",outgoing)\n",
    "        \n",
    "        self.add_hidden(incoming,outgoing)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "root = './data'\n",
    "    \n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.1307     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_names = [str(i%28) +\",\"+ str(int(i/28)) for i in range(28*28)]\n",
    "out_names = [str(i) for i in range(10)]\n",
    "imgs,labels = iter(train_loader).next()\n",
    "size = 28*28\n",
    "gnn = graph_neural_network(size,10,h_edges=5,in_names=in_names,out_names=out_names)\n",
    "imshow(torchvision.utils.make_grid(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total trainning batch number: 15000\n",
      "==>>> total testing batch number: 2500\n"
     ]
    }
   ],
   "source": [
    "print('==>>> total trainning batch number: {}'.format(len(train_loader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 28*28\n",
    "gnn = graph_neural_network(size,10,h_edges=5,in_names=in_names,out_names=out_names)\n",
    "gnn2 = graph_neural_network(size,10,h_edges=5)\n",
    "fc = nn.Sequential(nn.Linear(size,50),nn.ReLU(),nn.Linear(50,10))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [0,0]\n",
    "tags = [\"GNN cos\", \"GNN rand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> GNN cos: epoch 0, batch index: 100, test loss: 2.138481, acc: 0.268\n",
      "==>>> GNN cos: epoch 0, batch index: 200, test loss: 2.138736, acc: 0.244\n",
      "==>>> GNN cos: epoch 0, batch index: 300, test loss: 2.226376, acc: 0.250\n",
      "==>>> GNN cos: epoch 0, batch index: 400, test loss: 2.288271, acc: 0.247\n",
      "==>>> GNN cos: epoch 0, batch index: 500, test loss: 2.206455, acc: 0.249\n",
      "==>>> GNN cos: epoch 0, batch index: 600, test loss: 2.206765, acc: 0.247\n",
      "==>>> GNN cos: epoch 0, batch index: 700, test loss: 2.196396, acc: 0.246\n",
      "==>>> GNN cos: epoch 0, batch index: 800, test loss: 2.255504, acc: 0.243\n",
      "==>>> GNN cos: epoch 0, batch index: 900, test loss: 2.256097, acc: 0.242\n",
      "==>>> GNN cos: epoch 0, batch index: 1000, test loss: 2.274113, acc: 0.242\n",
      "==>>> GNN cos: epoch 0, batch index: 1100, test loss: 2.220385, acc: 0.244\n",
      "==>>> GNN cos: epoch 0, batch index: 1200, test loss: 2.283092, acc: 0.241\n",
      "==>>> GNN cos: epoch 0, batch index: 1300, test loss: 2.190168, acc: 0.241\n",
      "==>>> GNN cos: epoch 0, batch index: 1400, test loss: 2.170526, acc: 0.246\n",
      "==>>> GNN cos: epoch 0, batch index: 1500, test loss: 2.198144, acc: 0.247\n",
      "==>>> GNN cos: epoch 0, batch index: 1600, test loss: 2.225013, acc: 0.250\n",
      "==>>> GNN cos: epoch 0, batch index: 1700, test loss: 2.156651, acc: 0.251\n",
      "==>>> GNN cos: epoch 0, batch index: 1800, test loss: 2.258153, acc: 0.250\n",
      "==>>> GNN cos: epoch 0, batch index: 1900, test loss: 2.237305, acc: 0.249\n",
      "==>>> GNN cos: epoch 0, batch index: 2000, test loss: 2.215212, acc: 0.247\n",
      "==>>> GNN cos: epoch 0, batch index: 2100, test loss: 2.179816, acc: 0.247\n",
      "==>>> GNN cos: epoch 0, batch index: 2200, test loss: 2.192582, acc: 0.249\n",
      "==>>> GNN cos: epoch 0, batch index: 2300, test loss: 2.256676, acc: 0.249\n",
      "==>>> GNN cos: epoch 0, batch index: 2400, test loss: 2.216757, acc: 0.248\n",
      "==>>> GNN cos: epoch 0, batch index: 2500, test loss: 2.210456, acc: 0.247\n",
      "==>>> GNN rand: epoch 0, batch index: 100, test loss: 2.241764, acc: 0.228\n",
      "==>>> GNN rand: epoch 0, batch index: 200, test loss: 2.231008, acc: 0.203\n",
      "==>>> GNN rand: epoch 0, batch index: 300, test loss: 2.265374, acc: 0.205\n",
      "==>>> GNN rand: epoch 0, batch index: 400, test loss: 2.275402, acc: 0.204\n",
      "==>>> GNN rand: epoch 0, batch index: 500, test loss: 2.221475, acc: 0.203\n",
      "==>>> GNN rand: epoch 0, batch index: 600, test loss: 2.223773, acc: 0.205\n",
      "==>>> GNN rand: epoch 0, batch index: 700, test loss: 2.250368, acc: 0.205\n",
      "==>>> GNN rand: epoch 0, batch index: 800, test loss: 2.224043, acc: 0.205\n",
      "==>>> GNN rand: epoch 0, batch index: 900, test loss: 2.188576, acc: 0.206\n",
      "==>>> GNN rand: epoch 0, batch index: 1000, test loss: 2.235108, acc: 0.205\n",
      "==>>> GNN rand: epoch 0, batch index: 1100, test loss: 2.265076, acc: 0.208\n",
      "==>>> GNN rand: epoch 0, batch index: 1200, test loss: 2.231852, acc: 0.212\n",
      "==>>> GNN rand: epoch 0, batch index: 1300, test loss: 2.196424, acc: 0.218\n",
      "==>>> GNN rand: epoch 0, batch index: 1400, test loss: 2.242827, acc: 0.221\n",
      "==>>> GNN rand: epoch 0, batch index: 1500, test loss: 2.244473, acc: 0.222\n",
      "==>>> GNN rand: epoch 0, batch index: 1600, test loss: 2.172504, acc: 0.224\n",
      "==>>> GNN rand: epoch 0, batch index: 1700, test loss: 2.197483, acc: 0.227\n",
      "==>>> GNN rand: epoch 0, batch index: 1800, test loss: 2.188283, acc: 0.227\n",
      "==>>> GNN rand: epoch 0, batch index: 1900, test loss: 2.175711, acc: 0.227\n",
      "==>>> GNN rand: epoch 0, batch index: 2000, test loss: 2.121038, acc: 0.229\n",
      "==>>> GNN rand: epoch 0, batch index: 2100, test loss: 2.230582, acc: 0.232\n",
      "==>>> GNN rand: epoch 0, batch index: 2200, test loss: 2.168345, acc: 0.233\n",
      "==>>> GNN rand: epoch 0, batch index: 2300, test loss: 2.162215, acc: 0.236\n",
      "==>>> GNN rand: epoch 0, batch index: 2400, test loss: 2.121418, acc: 0.238\n",
      "==>>> GNN rand: epoch 0, batch index: 2500, test loss: 2.196733, acc: 0.238\n",
      "tensor([[ 3.3015e-05,  2.9550e-02,  1.8032e-02,  1.9557e-04,  3.4908e-12,\n",
      "          9.5183e-01,  1.1947e-05,  1.9545e-04,  3.7271e-05,  1.1088e-04],\n",
      "        [ 5.2573e-01,  8.2624e-02,  5.3099e-03,  5.4681e-04,  5.5673e-09,\n",
      "          3.8480e-01,  3.3405e-05,  5.4648e-04,  1.0421e-04,  3.1002e-04],\n",
      "        [ 1.2392e-02,  1.8475e-01,  5.3421e-01,  1.2227e-03,  1.2130e-10,\n",
      "          2.6520e-01,  7.4697e-05,  1.2220e-03,  2.3302e-04,  6.9321e-04],\n",
      "        [ 1.2464e-03,  1.1730e-01,  7.0978e-02,  7.7627e-04,  1.0444e-07,\n",
      "          8.0829e-01,  4.7423e-05,  7.7580e-04,  1.4794e-04,  4.4011e-04]])\n",
      "tensor([[-1.0735e-06, -9.4178e-04, -5.7924e-04, -6.3580e-06, -1.1350e-13,\n",
      "          1.5677e-03, -3.8846e-07, -6.3541e-06, -1.2118e-06, -3.1324e-05],\n",
      "        [ 5.2213e-02,  7.0945e-03,  4.4640e-04,  4.5912e-05,  4.6738e-10,\n",
      "         -5.9884e-02,  2.8045e-06,  4.5884e-05,  8.7488e-06,  2.6028e-05],\n",
      "        [ 4.3913e-04, -3.8858e-02,  2.7180e-02,  4.3022e-05,  4.2647e-12,\n",
      "          1.1117e-02,  2.6263e-06,  4.2996e-05,  8.1938e-06,  2.4383e-05],\n",
      "        [-3.3909e-04, -2.2717e-03, -1.4536e-03, -1.7131e-05, -2.3067e-09,\n",
      "          4.1126e-03, -1.0473e-06, -1.7121e-05, -3.2669e-06, -9.7159e-06]])\n",
      "torch.Size([5000, 794]) torch.Size([5000, 1])\n",
      "['-0.33(refined-stereotyped-koel-of-correction)', '-0.33(18,15)', '-0.33(17,15)', '-0.32(18,14)', '-0.32(17,14)'] -> ['0']\n",
      "tensor([[ 3.1125e-07,  7.0950e-01,  1.3778e-01,  4.6955e-03,  6.3023e-12,\n",
      "          1.3948e-01,  2.8686e-04,  4.6927e-03,  8.9486e-04,  2.6621e-03],\n",
      "        [ 7.1406e-01,  1.3548e-01,  1.2177e-01,  8.9659e-04,  8.3826e-08,\n",
      "          2.6161e-02,  5.4774e-05,  8.9605e-04,  1.7087e-04,  5.0832e-04],\n",
      "        [ 9.5591e-03,  3.5976e-01,  5.4389e-01,  2.3809e-03,  4.3001e-10,\n",
      "          8.0083e-02,  1.4545e-04,  2.3795e-03,  4.5375e-04,  1.3499e-03],\n",
      "        [ 8.0220e-01,  3.9634e-02,  2.4825e-02,  2.6230e-04,  4.0018e-09,\n",
      "          1.3261e-01,  1.6024e-05,  2.6214e-04,  4.9988e-05,  1.4871e-04]])\n",
      "tensor([[-4.9453e-09,  4.8790e-03, -1.7406e-03, -7.4120e-05, -1.0014e-13,\n",
      "         -1.7561e-03, -4.5560e-06, -1.2472e-03, -1.4201e-05, -4.2142e-05],\n",
      "        [ 2.5859e-02,  2.2279e-03, -2.8482e-02,  1.1896e-05,  1.1105e-09,\n",
      "          3.6187e-04,  7.2572e-07,  1.1889e-05,  2.2643e-06,  6.7400e-06],\n",
      "        [ 7.4381e-04, -5.8563e-02,  5.0936e-02,  1.8488e-04,  3.3368e-11,\n",
      "          6.3627e-03,  1.1287e-05,  1.8477e-04,  3.5215e-05,  1.0479e-04],\n",
      "        [-3.5771e-02,  7.1103e-03,  4.4453e-03,  4.6826e-05,  7.1439e-10,\n",
      "          2.4083e-02,  2.8606e-06,  4.6798e-05,  8.9237e-06,  2.6548e-05]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-513-8823e378bb41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0msamples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_cossim_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mgnn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_random_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-507-bcf50718d7f3>\u001b[0m in \u001b[0;36madd_cossim_hidden\u001b[0;34m(self, X, y, out)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_cossim_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_err_act_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-507-bcf50718d7f3>\u001b[0m in \u001b[0;36mget_err_act_vectors\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-507-bcf50718d7f3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, log)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(120):\n",
    "    for i in range(10):\n",
    "        gnn.add_random_hidden()\n",
    "        gnn2.add_random_hidden()\n",
    "    gnn.create_nn()\n",
    "    models[0] = gnn.nn\n",
    "    gnn2.create_nn()\n",
    "    models[1] = gnn2.nn\n",
    "    for model, tag in zip(models,tags):\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        # trainning\n",
    "        ave_loss = 0\n",
    "        for batch_idx, (x, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            x = x.view(batch_size,-1)\n",
    "            out = F.softmax(model(x),dim=1)\n",
    "            loss = criterion(out, target)\n",
    "            ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            #    print('==>>> {}: epoch {}, batch index: {}, train loss: {:.6f}'.format(tag,\n",
    "            #        epoch, batch_idx+1, ave_loss))\n",
    "\n",
    "        # testing\n",
    "        correct_cnt, ave_loss = 0, 0\n",
    "        total_cnt = 0\n",
    "        for batch_idx, (x, target) in enumerate(test_loader):\n",
    "            x = x.view(batch_size,-1)\n",
    "            out = F.softmax(model(x),dim=1)\n",
    "            loss = criterion(out, target)\n",
    "            _, pred_label = torch.max(out.data, 1)\n",
    "            total_cnt += x.data.size()[0]\n",
    "            correct_cnt += int((pred_label == target.data).sum())\n",
    "            # smooth average\n",
    "            ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "\n",
    "            if(batch_idx+1) == len(test_loader):\n",
    "                print('==>>> {}: epoch {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(tag,\n",
    "                    epoch, batch_idx+1, ave_loss, correct_cnt * 1.0 / float(total_cnt)))\n",
    "\n",
    "    gnn.update_graph()\n",
    "    for i in range(10):\n",
    "        loader = iter(train_loader)\n",
    "        samples = 0\n",
    "        X, y = [], []\n",
    "        while(samples < 5000):\n",
    "            X_t, y_t = loader.next()\n",
    "            X.append(X_t.view(batch_size,-1))\n",
    "            y.append(y_t)\n",
    "            samples += batch_size\n",
    "        samples = 0\n",
    "        gnn.add_cossim_hidden(X,y,out=i)\n",
    "        gnn2.add_random_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in zero_ins:\n",
    "    if len(z) == 2:\n",
    "        print(z[0],z[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in one_ins:\n",
    "    if len(o) == 2:\n",
    "        print(o[0],o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
