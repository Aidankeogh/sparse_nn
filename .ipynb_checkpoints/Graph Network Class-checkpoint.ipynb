{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import write_dot, graphviz_layout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from coolname import generate_slug as name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graph_neural_network():\n",
    "    def __init__(self,n_in,n_out):\n",
    "        self.G = nx.DiGraph()\n",
    "        self.outputs = \n",
    "        \n",
    "    def get_layers(self):\n",
    "        G = self.G\n",
    "        G2 = nx.topological_sort(G)\n",
    "        max_layer = 0\n",
    "        for n in G2:\n",
    "            G.node[n]['layer'] = max([G.node[k[0]]['layer'] for k in G.in_edges(n)] + [-1]) + 1\n",
    "            max_layer = max(max_layer,G.node[n]['layer'])\n",
    "\n",
    "        layers = [[] for _ in range(max_layer + 1)]\n",
    "\n",
    "        for n in G:\n",
    "            layers[G.node[n]['layer']].append(n)\n",
    "\n",
    "        flat_idx = 0\n",
    "        for i, layer in enumerate(layers):\n",
    "            for j, n in enumerate(layer):\n",
    "                G.node[n]['idx'] = j\n",
    "                G.node[n]['flat_idx'] = flat_idx\n",
    "                flat_idx += 1\n",
    "        return layers\n",
    "    \n",
    "    def get_output_idxes(self):\n",
    "        outputs = self.outputs\n",
    "        G = self.G\n",
    "        out_idxes = []\n",
    "        for n in G:\n",
    "            if(n in outputs):\n",
    "                out_idxes.append(G.node[n]['flat_idx'])\n",
    "        return out_idxes\n",
    "    \n",
    "    def graph_to_weights(self):\n",
    "        G = self.G\n",
    "        outputs = self.outputs\n",
    "        \n",
    "        layers = self.get_layers()\n",
    "        mask = []    \n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        n_nodes = 0\n",
    "        for i in range(len(layers) - 1):\n",
    "            n_nodes += len(layers[i])\n",
    "            mask.append(np.zeros((len(layers[i+1]),n_nodes)))\n",
    "            biases.append(np.zeros((len(layers[i+1]))))\n",
    "            weights.append(np.zeros((len(layers[i+1]),n_nodes)))\n",
    "\n",
    "            for j, node1 in enumerate(layers[i+1]):\n",
    "                biases[i][j] = G.node[node1]['bias']\n",
    "                for node0, _ in G.in_edges(node1): \n",
    "                    u = G.node[node0]['flat_idx']\n",
    "                    v = G.node[node1]['idx']\n",
    "                    mask[i][v,u] = 1\n",
    "                    weights[i][v,u] = G[node0][node1]['weight']\n",
    "\n",
    "        n_nodes += len(layers[-1])\n",
    "        out_idxes = self.get_output_idxes(G, outputs)\n",
    "        lastlayer = np.zeros((len(out_idxes),n_nodes))\n",
    "        for v, u in enumerate(out_idxes): #identity layer mapping to output neurons\n",
    "            lastlayer[v,u] = 1\n",
    "\n",
    "        mask.append(lastlayer)\n",
    "        weights.append(lastlayer)\n",
    "        biases.append(np.zeros(len(out_idxes)))\n",
    "\n",
    "        return weights, biases, mask\n",
    "    \n",
    "    def weights_to_graph(self,weights,biases):\n",
    "        G = self.G\n",
    "        layers = get_layers(G)\n",
    "        for i in range(len(layers) - 1):\n",
    "            for j, node1 in enumerate(layers[i+1]):\n",
    "                G.node[node1]['bias'] = biases[i][j]\n",
    "                for node0, _ in G.in_edges(node1): \n",
    "                    u = G.node[node0]['flat_idx']\n",
    "                    v = G.node[node1]['idx']\n",
    "                    G[node0][node1]['weight'] = weights[i][v,u]\n",
    "                    \n",
    "def zero_grad(self, grad_input, grad_output):\n",
    "    temp = list(grad_input)\n",
    "    temp[2] *= torch.transpose(self.mask,0, 1)\n",
    "    return tuple(temp)\n",
    "\n",
    "class SparseNet(nn.Module):\n",
    "    def __init__(self, wieghts, biases, masks):\n",
    "        super(SparseNet, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.masks = masks\n",
    "        i = 0\n",
    "        for w, b, m in zip(weights,biases,masks):\n",
    "            temp = nn.Linear(w.shape[1],w.shape[0])\n",
    "            temp.weight.data = torch.from_numpy(w.astype(np.float32))\n",
    "            temp.bias.data = torch.from_numpy(b.astype(np.float32))\n",
    "            temp.mask = torch.from_numpy(m.astype(np.float32))\n",
    "            temp.num = i\n",
    "            temp.register_backward_hook(zero_grad)\n",
    "            self.layers.append(temp)\n",
    "            i += 1\n",
    "        self.layers[-1].requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            y = l(x)\n",
    "            x = torch.cat((x,y),1)\n",
    "            x = F.relu(x)\n",
    "        return y\n",
    "\n",
    "    def dumpweights(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for l in self.layers:\n",
    "            weights.append(l.weight.data.numpy().astype(np.float64))\n",
    "            biases.append(l.bias.data.numpy().astype(np.float64))\n",
    "        return weights, biases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
