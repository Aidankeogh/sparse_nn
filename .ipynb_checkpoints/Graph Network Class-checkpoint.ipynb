{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import write_dot, graphviz_layout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from coolname import generate_slug as name\n",
    "import yaml\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(self, grad_input, grad_output):\n",
    "    temp = list(grad_input)\n",
    "    #Temp[2] is the gradients to the weights, zero them out so they cant change. \n",
    "    temp[2] *= torch.transpose(self.mask,0, 1)\n",
    "    return tuple(temp)\n",
    "\n",
    "class SparseNet(nn.Module):\n",
    "    def __init__(self, weights, biases, masks):\n",
    "        super(SparseNet, self).__init__()\n",
    "        self.act_log = None\n",
    "        self.err_log = None\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.masks = masks\n",
    "        self.out = None\n",
    "        for w, b, m in zip(weights,biases,masks):\n",
    "            temp = nn.Linear(w.shape[1],w.shape[0])\n",
    "            temp.weight.data = torch.from_numpy(w.astype(np.float32))\n",
    "            temp.bias.data = torch.from_numpy(b.astype(np.float32))\n",
    "            temp.mask = torch.from_numpy(m.astype(np.float32))\n",
    "            temp.register_backward_hook(zero_grad)\n",
    "            self.layers.append(temp)\n",
    "\n",
    "    def forward(self, x, log=False):\n",
    "        self.log = log\n",
    "        \n",
    "        for l in self.layers[:-1]:\n",
    "            y = l(x)\n",
    "            x = torch.cat((x,y),1)\n",
    "            x = F.relu(x)\n",
    "        self.out = self.layers[-1](x)\n",
    "        \n",
    "        if(log):\n",
    "            self.out.retain_grad()\n",
    "            if self.act_log is None:\n",
    "                self.act_log = x.detach()\n",
    "            else:\n",
    "                self.act_log = torch.cat((self.act_log,x.detach()), dim=0)\n",
    "            \n",
    "        return self.out\n",
    "    \n",
    "    def clear(self):\n",
    "        self.act_log = None\n",
    "        self.err_log = None\n",
    "\n",
    "    def dumpweights(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for l in self.layers:\n",
    "            weights.append(l.weight.data.numpy().astype(np.float64))\n",
    "            biases.append(l.bias.data.numpy().astype(np.float64))\n",
    "        return weights, biases\n",
    "\n",
    "class graph_neural_network():\n",
    "    def __init__(self,n_in,n_out,h_edges=3):\n",
    "        self.G = nx.DiGraph()\n",
    "        self.h_edges = h_edges\n",
    "        self.inputs = [\"(in)\"+name() for i in range(n_in)]\n",
    "        self.outputs = [\"(out)\"+name() for i in range(n_out)]\n",
    "        self.flat = None\n",
    "        for i in self.inputs:\n",
    "            self.G.add_node(i) \n",
    "        for o in self.outputs:\n",
    "            self.G.add_node(o)\n",
    "            self.G.node[o]['bias'] = np.random.normal(0,1) \n",
    "        self.hidden = []\n",
    "    \n",
    "    def add_hidden(self,incoming,outgoing):\n",
    "        h = name()\n",
    "        print(incoming,outgoing)\n",
    "        edges = []\n",
    "        for i in incoming:\n",
    "            edges.append((i,h,np.random.normal(0,1.0/self.h_edges)))\n",
    "        \n",
    "        approx_xavier = len(self.outputs) * 1.0/ (1+len(self.hidden))\n",
    "        \n",
    "        for o in outgoing:\n",
    "            edges.append((h,o,np.random.normal(0,approx_xavier)))\n",
    "            \n",
    "        self.G.add_weighted_edges_from(edges)\n",
    "        self.G.node[h]['bias'] = random.uniform(-.1,.1)\n",
    "        self.hidden.append(h)\n",
    "    \n",
    "    def add_random_hidden(self):\n",
    "        incoming = [random.choice(self.inputs+self.hidden) for i in range(self.h_edges)]\n",
    "        outgoing = [random.choice(self.outputs)]\n",
    "        self.add_hidden(incoming,outgoing)\n",
    "    \n",
    "    def get_layers(self):\n",
    "        G = self.G\n",
    "        G2 = nx.topological_sort(G)\n",
    "        max_layer = 0\n",
    "        for n in G2:\n",
    "            if n not in self.outputs:\n",
    "                G.node[n]['layer'] = max([G.node[k[0]]['layer'] for k in G.in_edges(n)] + [-1]) + 1\n",
    "                max_layer = max(max_layer,G.node[n]['layer'])\n",
    "        for n in self.outputs:\n",
    "            G.node[n]['layer'] = max_layer + 1\n",
    "\n",
    "        layers = [[] for _ in range(max_layer + 2)]\n",
    "\n",
    "        for n in G:\n",
    "            layers[G.node[n]['layer']].append(n)\n",
    "\n",
    "        self.flat = []\n",
    "        flat_idx = 0\n",
    "        for i, layer in enumerate(layers):\n",
    "            for j, n in enumerate(layer):\n",
    "                G.node[n]['idx'] = j\n",
    "                G.node[n]['flat_idx'] = flat_idx\n",
    "                self.flat.append(n)\n",
    "                flat_idx += 1\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def get_output_idxes(self):\n",
    "        outputs = self.outputs\n",
    "        G = self.G\n",
    "        out_idxes = []\n",
    "        for n in G:\n",
    "            if(n in outputs):\n",
    "                out_idxes.append(G.node[n]['flat_idx'])\n",
    "        return out_idxes\n",
    "    \n",
    "    def get_weights(self):\n",
    "        G = self.G\n",
    "        outputs = self.outputs\n",
    "        \n",
    "        layers = self.get_layers()\n",
    "        mask = []    \n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        n_nodes = 0\n",
    "        for i in range(len(layers) - 1):\n",
    "            n_nodes += len(layers[i])\n",
    "            mask.append(np.zeros((len(layers[i+1]),n_nodes)))\n",
    "            biases.append(np.zeros((len(layers[i+1]))))\n",
    "            weights.append(np.zeros((len(layers[i+1]),n_nodes)))\n",
    "\n",
    "            for j, node1 in enumerate(layers[i+1]):\n",
    "                biases[i][j] = G.node[node1]['bias']\n",
    "                for node0, _ in G.in_edges(node1): \n",
    "                    u = G.node[node0]['flat_idx']\n",
    "                    v = G.node[node1]['idx']\n",
    "                    mask[i][v,u] = 1\n",
    "                    weights[i][v,u] = G[node0][node1]['weight']\n",
    "\n",
    "        return weights, biases, mask\n",
    "    \n",
    "    def set_weights(self,weights,biases):\n",
    "        layers = self.get_layers()\n",
    "        for i in range(len(layers) - 1):\n",
    "            for j, node1 in enumerate(layers[i+1]):\n",
    "                self.G.node[node1]['bias'] = biases[i][j]\n",
    "                for node0, _ in self.G.in_edges(node1): \n",
    "                    u = self.G.node[node0]['flat_idx']\n",
    "                    v = self.G.node[node1]['idx']\n",
    "                    self.G[node0][node1]['weight'] = weights[i][v,u]\n",
    "                    \n",
    "    def create_nn(self):\n",
    "        w, b, m = self.get_weights()\n",
    "        self.nn = SparseNet(w,b,m)\n",
    "    \n",
    "    def update_graph(self):\n",
    "        weights = [l.weight.data.numpy() for l in self.nn.layers]\n",
    "        biases = [l.bias.data.numpy() for l in self.nn.layers]\n",
    "        self.set_weights(weights,biases)\n",
    "    \n",
    "    def get_err_act_vectors(self,X,y):\n",
    "        gnn.nn.clear()\n",
    "        for j in range(len(X)):\n",
    "            out = gnn.nn(X[j],log=True) \n",
    "            loss = criterion(out,y[j])\n",
    "            loss.backward()\n",
    "\n",
    "            if gnn.nn.err_log is None:\n",
    "                gnn.nn.err_log = gnn.nn.out.grad\n",
    "            else:\n",
    "                gnn.nn.err_log = torch.cat((gnn.nn.err_log,gnn.nn.out.grad), dim=0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        return gnn.nn.act_log, gnn.nn.err_log\n",
    "    \n",
    "    def add_cossim_hidden(self,X,y):\n",
    "        act, err = self.get_err_act_vectors(X,y)\n",
    "        \n",
    "        out = random.randint(0,len(self.outputs)-1)\n",
    "        err = err[:,out:out+1]\n",
    "        \n",
    "        incoming = []\n",
    "        \n",
    "        cossim = F.cosine_similarity(err,act,dim=0)\n",
    "        _, choices = torch.topk(cossim.abs(), self.h_edges, dim=0)\n",
    "        \n",
    "        for choice in choices:\n",
    "            incoming.append(self.flat[choice])\n",
    "        outgoing = [self.outputs[out]]\n",
    "        self.add_hidden(incoming,outgoing)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "data = datasets.load_iris()\n",
    "X = [torch.FloatTensor([d]) for d in data.data]\n",
    "y = [torch.FloatTensor([[d]]) for d in data.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = graph_neural_network(len(X[0][0]),len(y[0][0]),h_edges=5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-edd756ad8689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "avgloss = 0\n",
    "maxnodes = 50\n",
    "split = int(len(X) * 0.8)\n",
    "for k in range(maxnodes+1):\n",
    "    gnn.create_nn()\n",
    "    optimizer = optim.Adam(gnn.nn.parameters(), lr=0.001)\n",
    "    \n",
    "    for j in range(50000):\n",
    "        i = j % split\n",
    "        out = gnn.nn(X[i])\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(out,y[i])\n",
    "        avgloss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if j % 5000 == 4999:\n",
    "            print(avgloss/5000)\n",
    "            avgloss = 0\n",
    "            \n",
    "    SSE = 0\n",
    "    for v_X, v_y in zip(X[400:],y[400:]):\n",
    "        out = gnn.nn(v_X)\n",
    "        SSE += (out[0][0] - v_y[0][0])**2\n",
    "    print(\"v_SSE:\", SSE/len(X[split:]))\n",
    "    if k != maxnodes:\n",
    "        print(\"Adding node\")\n",
    "        gnn.update_graph()\n",
    "        Xh, yh = shuffle(X[:split],y[:split])\n",
    "        gnn.add_cossim_hidden(Xh[:150],yh[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn.choose_connectons(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
