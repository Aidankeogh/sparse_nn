{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import write_dot, graphviz_layout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from coolname import generate_slug as name\n",
    "import yaml\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_ins = []\n",
    "one_ins = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(self, grad_input, grad_output):\n",
    "    temp = list(grad_input)\n",
    "    #Temp[2] is the gradients to the weights, zero them out so they cant change. \n",
    "    temp[2] *= torch.transpose(self.mask,0, 1)\n",
    "    return tuple(temp)\n",
    "\n",
    "class SparseNet(nn.Module):\n",
    "    def __init__(self, weights, biases, masks):\n",
    "        super(SparseNet, self).__init__()\n",
    "        self.act_log = None\n",
    "        self.err_log = None\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.masks = masks\n",
    "        self.out = None\n",
    "        for w, b, m in zip(weights,biases,masks):\n",
    "            temp = nn.Linear(w.shape[1],w.shape[0])\n",
    "            temp.weight.data = torch.from_numpy(w.astype(np.float32))\n",
    "            temp.bias.data = torch.from_numpy(b.astype(np.float32))\n",
    "            temp.mask = torch.from_numpy(m.astype(np.float32))\n",
    "            temp.register_backward_hook(zero_grad)\n",
    "            self.layers.append(temp)\n",
    "\n",
    "    def forward(self, x, log=False):\n",
    "        self.log = log\n",
    "        \n",
    "        for l in self.layers[:-1]:\n",
    "            y = l(x)\n",
    "            x = torch.cat((x,y),1)\n",
    "            x = F.relu(x)\n",
    "        self.out = self.layers[-1](x)\n",
    "        \n",
    "        if(log):\n",
    "            self.out.retain_grad()\n",
    "            if self.act_log is None:\n",
    "                self.act_log = x.detach()\n",
    "            else:\n",
    "                self.act_log = torch.cat((self.act_log,x.detach()), dim=0)\n",
    "            \n",
    "        return self.out\n",
    "    \n",
    "    def clear(self):\n",
    "        self.act_log = None\n",
    "        self.err_log = None\n",
    "\n",
    "    def dumpweights(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for l in self.layers:\n",
    "            weights.append(l.weight.data.numpy().astype(np.float64))\n",
    "            biases.append(l.bias.data.numpy().astype(np.float64))\n",
    "        return weights, biases\n",
    "\n",
    "class graph_neural_network():\n",
    "    def __init__(self,n_in,n_out,h_edges=3,in_names = None, out_names = None):\n",
    "        self.G = nx.DiGraph()\n",
    "        self.h_edges = h_edges\n",
    "        self.inputs = in_names\n",
    "        self.outputs = out_names\n",
    "        if in_names is None:\n",
    "            self.inputs = [\"(in)\"+name() for i in range(n_in)]\n",
    "        if out_names is None:\n",
    "            self.outputs = [\"(out)\"+name() for i in range(n_out)]\n",
    "        self.flat = None\n",
    "        for i in self.inputs:\n",
    "            self.G.add_node(i) \n",
    "        for o in self.outputs:\n",
    "            self.G.add_node(o)\n",
    "            self.G.node[o]['bias'] = np.random.normal(0,1) \n",
    "        self.hidden = []\n",
    "    \n",
    "    def add_hidden(self,incoming,outgoing):\n",
    "        h = name()\n",
    "        #print(incoming,outgoing)\n",
    "        edges = []\n",
    "        for i in incoming:\n",
    "            edges.append((i,h,np.random.normal(0,1.0/self.h_edges)))\n",
    "        \n",
    "        approx_xavier = len(self.outputs) * 1.0/ (1+len(self.hidden))\n",
    "        \n",
    "        for o in outgoing:\n",
    "            edges.append((h,o,np.random.normal(0,approx_xavier)))\n",
    "            \n",
    "        self.G.add_weighted_edges_from(edges)\n",
    "        self.G.node[h]['bias'] = random.uniform(-.1,.1)\n",
    "        self.hidden.append(h)\n",
    "    \n",
    "    def add_random_hidden(self):\n",
    "        incoming = [random.choice(self.inputs+self.hidden) for i in range(self.h_edges)]\n",
    "        outgoing = [random.choice(self.outputs)]\n",
    "        self.add_hidden(incoming,outgoing)\n",
    "    \n",
    "    def get_layers(self):\n",
    "        G = self.G\n",
    "        G2 = nx.topological_sort(G)\n",
    "        max_layer = 0\n",
    "        for n in G2:\n",
    "            if n not in self.outputs:\n",
    "                G.node[n]['layer'] = max([G.node[k[0]]['layer'] for k in G.in_edges(n)] + [-1]) + 1\n",
    "                max_layer = max(max_layer,G.node[n]['layer'])\n",
    "        for n in self.outputs:\n",
    "            G.node[n]['layer'] = max_layer + 1\n",
    "\n",
    "        layers = [[] for _ in range(max_layer + 2)]\n",
    "\n",
    "        for n in G:\n",
    "            layers[G.node[n]['layer']].append(n)\n",
    "\n",
    "        self.flat = []\n",
    "        flat_idx = 0\n",
    "        for i, layer in enumerate(layers):\n",
    "            for j, n in enumerate(layer):\n",
    "                G.node[n]['idx'] = j\n",
    "                G.node[n]['flat_idx'] = flat_idx\n",
    "                self.flat.append(n)\n",
    "                flat_idx += 1\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def get_output_idxes(self):\n",
    "        outputs = self.outputs\n",
    "        G = self.G\n",
    "        out_idxes = []\n",
    "        for n in G:\n",
    "            if(n in outputs):\n",
    "                out_idxes.append(G.node[n]['flat_idx'])\n",
    "        return out_idxes\n",
    "    \n",
    "    def get_weights(self):\n",
    "        G = self.G\n",
    "        outputs = self.outputs\n",
    "        \n",
    "        layers = self.get_layers()\n",
    "        mask = []    \n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        n_nodes = 0\n",
    "        for i in range(len(layers) - 1):\n",
    "            n_nodes += len(layers[i])\n",
    "            mask.append(np.zeros((len(layers[i+1]),n_nodes)))\n",
    "            biases.append(np.zeros((len(layers[i+1]))))\n",
    "            weights.append(np.zeros((len(layers[i+1]),n_nodes)))\n",
    "\n",
    "            for j, node1 in enumerate(layers[i+1]):\n",
    "                biases[i][j] = G.node[node1]['bias']\n",
    "                for node0, _ in G.in_edges(node1): \n",
    "                    u = G.node[node0]['flat_idx']\n",
    "                    v = G.node[node1]['idx']\n",
    "                    mask[i][v,u] = 1\n",
    "                    weights[i][v,u] = G[node0][node1]['weight']\n",
    "\n",
    "        return weights, biases, mask\n",
    "    \n",
    "    def set_weights(self,weights,biases):\n",
    "        layers = self.get_layers()\n",
    "        for i in range(len(layers) - 1):\n",
    "            for j, node1 in enumerate(layers[i+1]):\n",
    "                self.G.node[node1]['bias'] = biases[i][j]\n",
    "                for node0, _ in self.G.in_edges(node1): \n",
    "                    u = self.G.node[node0]['flat_idx']\n",
    "                    v = self.G.node[node1]['idx']\n",
    "                    self.G[node0][node1]['weight'] = weights[i][v,u]\n",
    "                    \n",
    "    def create_nn(self):\n",
    "        w, b, m = self.get_weights()\n",
    "        self.nn = SparseNet(w,b,m)\n",
    "    \n",
    "    def update_graph(self):\n",
    "        weights = [l.weight.data.numpy() for l in self.nn.layers]\n",
    "        biases = [l.bias.data.numpy() for l in self.nn.layers]\n",
    "        self.set_weights(weights,biases)\n",
    "    \n",
    "    def get_err_act_vectors(self,X,y):\n",
    "        gnn.nn.clear()\n",
    "        for j in range(len(X)):\n",
    "            out = gnn.nn(X[j],log=True) \n",
    "            out = F.softmax(out,dim=1)\n",
    "            loss = criterion(out,y[j])\n",
    "            loss.backward()\n",
    "\n",
    "            if gnn.nn.err_log is None:\n",
    "                gnn.nn.err_log = gnn.nn.out.grad\n",
    "            else:\n",
    "                gnn.nn.err_log = torch.cat((gnn.nn.err_log,gnn.nn.out.grad), dim=0)\n",
    "                \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        return gnn.nn.act_log, gnn.nn.err_log\n",
    "    \n",
    "    def add_cossim_hidden(self,X,y,out=None):\n",
    "        \n",
    "        act, err = self.get_err_act_vectors(X,y)\n",
    "        if out is None:\n",
    "            out = random.randint(0,len(self.outputs)-1)\n",
    "        err = err[:,out:out+1]\n",
    "        \n",
    "        #print(act.size(),err.size())\n",
    "        act = F.normalize(act)\n",
    "        err = F.normalize(err)\n",
    "        \n",
    "        incoming = []\n",
    "        toprint = []\n",
    "        cossim = F.cosine_similarity(err,act,dim=0)\n",
    "        _, choices = torch.topk(cossim.abs(),self.h_edges, dim=0)\n",
    "        for choice in choices:\n",
    "            incoming.append(self.flat[choice])\n",
    "            toprint.append((\"%.2f(\" % cossim[choice]) + self.flat[choice]+\")\")\n",
    "            if out == 0:\n",
    "                zero_ins.append(self.flat[choice].split(\",\"))\n",
    "            if out == 1:\n",
    "                one_ins.append(self.flat[choice].split(\",\"))\n",
    "            \n",
    "        outgoing = [self.outputs[out]]\n",
    "        print(toprint,\"->\",outgoing)\n",
    "        \n",
    "        self.add_hidden(incoming,outgoing)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "root = './data'\n",
    "    \n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.1307     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_names = [str(i%28) +\",\"+ str(int(i/28)) for i in range(28*28)]\n",
    "out_names = [str(i) for i in range(10)]\n",
    "imgs,labels = iter(train_loader).next()\n",
    "size = 28*28\n",
    "gnn = graph_neural_network(size,10,h_edges=5,in_names=in_names,out_names=out_names)\n",
    "imshow(torchvision.utils.make_grid(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total trainning batch number: 600\n",
      "==>>> total testing batch number: 100\n"
     ]
    }
   ],
   "source": [
    "print('==>>> total trainning batch number: {}'.format(len(train_loader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 28*28\n",
    "gnn = graph_neural_network(size,10,h_edges=5,in_names=in_names,out_names=out_names)\n",
    "gnn2 = graph_neural_network(size,10,h_edges=5)\n",
    "fc = nn.Sequential(nn.Linear(size,50),nn.ReLU(),nn.Linear(50,10))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [0,0]\n",
    "tags = [\"GNN cos\", \"GNN rand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> GNN cos: epoch 0, batch index: 100, test loss: 2.162555, acc: 0.299\n",
      "==>>> GNN rand: epoch 0, batch index: 100, test loss: 2.235752, acc: 0.196\n",
      "['0.25(12,19)', '-0.24(8,14)', '-0.24(8,13)', '-0.24(7,14)', '0.23(15,10)'] -> ['0']\n",
      "['0.34(unselfish-viridian-oriole-of-piety)', '0.32(14,5)', '0.32(20,18)', '0.32(20,17)', '0.31(15,5)'] -> ['1']\n",
      "['0.24(14,10)', '0.23(15,10)', '0.22(14,11)', '0.22(15,9)', '0.22(14,9)'] -> ['2']\n",
      "['0.26(13,19)', '0.25(12,20)', '0.25(12,19)', '0.24(13,20)', '0.23(11,20)'] -> ['3']\n",
      "['0.29(15,4)', '0.28(12,20)', '0.28(14,4)', '0.28(12,19)', '0.27(14,5)'] -> ['4']\n",
      "['0.30(14,5)', '0.30(15,5)', '0.27(14,4)', '0.26(15,4)', '0.26(12,19)'] -> ['5']\n",
      "['0.31(15,10)', '0.28(15,9)', '0.27(15,11)', '0.26(14,11)', '0.26(16,9)'] -> ['6']\n",
      "['0.46(18,19)', '0.45(18,18)', '0.45(17,19)', '0.45(17,20)', '0.44(19,18)'] -> ['7']\n",
      "['0.27(15,10)', '0.27(14,10)', '0.27(19,18)', '0.27(15,4)', '0.27(15,9)'] -> ['8']\n",
      "['0.29(12,20)', '0.29(15,4)', '0.28(14,4)', '0.28(14,5)', '0.28(16,4)'] -> ['9']\n",
      "==>>> GNN cos: epoch 1, batch index: 100, test loss: 2.065719, acc: 0.402\n",
      "==>>> GNN rand: epoch 1, batch index: 100, test loss: 2.151393, acc: 0.336\n",
      "['-0.27(9,12)', '-0.26(8,13)', '-0.26(9,11)', '-0.26(8,14)', '-0.26(9,13)'] -> ['0']\n",
      "['0.19(15,3)', '0.18(16,3)', '0.18(14,3)', '0.18(11,4)', '0.18(12,4)'] -> ['1']\n",
      "['0.25(14,11)', '0.24(15,10)', '0.23(14,12)', '0.23(15,11)', '0.22(14,10)'] -> ['2']\n",
      "['0.26(12,19)', '0.25(13,19)', '0.25(accomplished-calculating-chachalaca-of-opportunity)', '0.24(11,19)', '0.24(13,18)'] -> ['3']\n",
      "['0.27(accomplished-calculating-chachalaca-of-opportunity)', '0.24(12,19)', '0.24(14,4)', '0.24(15,10)', '0.23(13,4)'] -> ['4']\n",
      "['0.26(accomplished-calculating-chachalaca-of-opportunity)', '0.25(15,11)', '-0.25(9,13)', '-0.24(9,12)', '0.24(15,10)'] -> ['5']\n",
      "['0.25(15,10)', '-0.24(9,14)', '0.24(15,11)', '-0.23(9,13)', '-0.23(10,12)'] -> ['6']\n",
      "['0.34(quantum-boisterous-unicorn-of-fury)', '0.33(14,14)', '0.32(13,15)', '0.32(13,16)', '0.31(14,15)'] -> ['7']\n",
      "['0.26(accomplished-calculating-chachalaca-of-opportunity)', '0.24(12,19)', '0.24(15,10)', '0.24(15,11)', '0.23(13,19)'] -> ['8']\n",
      "['0.26(accomplished-calculating-chachalaca-of-opportunity)', '0.25(12,19)', '0.24(13,19)', '0.24(15,10)', '0.23(13,18)'] -> ['9']\n",
      "==>>> GNN cos: epoch 2, batch index: 100, test loss: 1.963519, acc: 0.497\n",
      "==>>> GNN rand: epoch 2, batch index: 100, test loss: 2.129590, acc: 0.345\n",
      "['0.32(dangerous-psychedelic-chihuahua-of-glee)', '0.30(sly-fat-rhino-from-pluto)', '0.27(accomplished-calculating-chachalaca-of-opportunity)', '0.27(courageous-flamingo-of-radical-drama)', '0.27(13,18)'] -> ['0']\n",
      "['0.24(20,18)', '0.23(19,18)', '0.23(20,17)', '0.22(19,19)', '0.22(unselfish-viridian-oriole-of-piety)'] -> ['1']\n",
      "['0.24(lovely-eggplant-impala-from-shambhala)', '0.24(14,10)', '0.23(21,15)', '0.23(15,10)', '0.22(courageous-flamingo-of-radical-drama)'] -> ['2']\n",
      "['0.29(accomplished-calculating-chachalaca-of-opportunity)', '0.27(dangerous-psychedelic-chihuahua-of-glee)', '0.26(12,19)', '0.26(12,20)', '0.25(11,19)'] -> ['3']\n",
      "['0.36(dangerous-psychedelic-chihuahua-of-glee)', '0.35(accomplished-calculating-chachalaca-of-opportunity)', '0.32(sly-fat-rhino-from-pluto)', '0.29(12,20)', '0.28(15,4)'] -> ['4']\n",
      "['0.37(dangerous-psychedelic-chihuahua-of-glee)', '0.34(accomplished-calculating-chachalaca-of-opportunity)', '0.33(sly-fat-rhino-from-pluto)', '0.29(15,10)', '0.29(14,4)'] -> ['5']\n",
      "['0.29(courageous-flamingo-of-radical-drama)', '0.28(15,10)', '0.26(16,9)', '0.25(15,9)', '0.25(tremendous-horned-hedgehog-of-security)'] -> ['6']\n",
      "['0.36(accomplished-calculating-chachalaca-of-opportunity)', '0.35(dangerous-psychedelic-chihuahua-of-glee)', '0.32(sly-fat-rhino-from-pluto)', '0.31(12,20)', '0.30(11,20)'] -> ['7']\n",
      "['0.36(dangerous-psychedelic-chihuahua-of-glee)', '0.35(accomplished-calculating-chachalaca-of-opportunity)', '0.32(sly-fat-rhino-from-pluto)', '0.29(15,4)', '0.28(14,4)'] -> ['8']\n",
      "['0.35(dangerous-psychedelic-chihuahua-of-glee)', '0.33(accomplished-calculating-chachalaca-of-opportunity)', '0.31(sly-fat-rhino-from-pluto)', '0.28(courageous-flamingo-of-radical-drama)', '0.28(15,10)'] -> ['9']\n",
      "==>>> GNN cos: epoch 3, batch index: 100, test loss: 1.927338, acc: 0.524\n",
      "==>>> GNN rand: epoch 3, batch index: 100, test loss: 2.067327, acc: 0.409\n",
      "['0.35(dangerous-psychedelic-chihuahua-of-glee)', '0.34(fair-righteous-dalmatian-of-imagination)', '0.33(jolly-loud-camel-of-variation)', '0.30(aboriginal-independent-owl-from-hyperborea)', '0.29(lush-rugged-stingray-of-triumph)'] -> ['0']\n",
      "['0.25(judicious-coral-fox-of-blizzard)', '0.24(20,18)', '0.24(7,19)', '0.24(7,20)', '0.23(20,17)'] -> ['1']\n",
      "['0.26(illegal-carrot-puffin-of-realization)', '0.24(14,10)', '0.24(lovely-eggplant-impala-from-shambhala)', '0.23(15,9)', '0.23(15,10)'] -> ['2']\n",
      "['0.30(accomplished-calculating-chachalaca-of-opportunity)', '0.29(electric-quick-slug-from-vega)', '0.29(functional-turquoise-rooster-of-enhancement)', '0.28(curvy-ubiquitous-bustard-of-effort)', '0.27(lush-rugged-stingray-of-triumph)'] -> ['3']\n",
      "['0.36(fair-righteous-dalmatian-of-imagination)', '0.36(dangerous-psychedelic-chihuahua-of-glee)', '0.35(jolly-loud-camel-of-variation)', '0.33(aboriginal-independent-owl-from-hyperborea)', '0.33(electric-quick-slug-from-vega)'] -> ['4']\n",
      "['0.37(fair-righteous-dalmatian-of-imagination)', '0.36(dangerous-psychedelic-chihuahua-of-glee)', '0.36(jolly-loud-camel-of-variation)', '0.35(aboriginal-independent-owl-from-hyperborea)', '0.34(curvy-ubiquitous-bustard-of-effort)'] -> ['5']\n",
      "['0.29(15,10)', '0.29(courageous-flamingo-of-radical-drama)', '0.27(dangerous-psychedelic-chihuahua-of-glee)', '0.27(16,9)', '0.27(15,9)'] -> ['6']\n",
      "['0.35(curvy-ubiquitous-bustard-of-effort)', '0.35(fair-righteous-dalmatian-of-imagination)', '0.35(accomplished-calculating-chachalaca-of-opportunity)', '0.34(aboriginal-independent-owl-from-hyperborea)', '0.34(electric-quick-slug-from-vega)'] -> ['7']\n",
      "['0.36(fair-righteous-dalmatian-of-imagination)', '0.36(dangerous-psychedelic-chihuahua-of-glee)', '0.35(jolly-loud-camel-of-variation)', '0.34(aboriginal-independent-owl-from-hyperborea)', '0.33(curvy-ubiquitous-bustard-of-effort)'] -> ['8']\n",
      "['0.38(fair-righteous-dalmatian-of-imagination)', '0.37(dangerous-psychedelic-chihuahua-of-glee)', '0.37(jolly-loud-camel-of-variation)', '0.36(aboriginal-independent-owl-from-hyperborea)', '0.36(curvy-ubiquitous-bustard-of-effort)'] -> ['9']\n",
      "==>>> GNN cos: epoch 4, batch index: 100, test loss: 1.874500, acc: 0.591\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-522-3003536f76b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# trainning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mave_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/local/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/local/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mnchannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(120):\n",
    "    for i in range(10):\n",
    "        gnn.add_random_hidden()\n",
    "        gnn2.add_random_hidden()\n",
    "    gnn.create_nn()\n",
    "    models[0] = gnn.nn\n",
    "    gnn2.create_nn()\n",
    "    models[1] = gnn2.nn\n",
    "    for model, tag in zip(models,tags):\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        # trainning\n",
    "        ave_loss = 0\n",
    "        for batch_idx, (x, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            x = x.view(batch_size,-1)\n",
    "            out = F.softmax(model(x),dim=1)\n",
    "            loss = criterion(out, target)\n",
    "            ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            #    print('==>>> {}: epoch {}, batch index: {}, train loss: {:.6f}'.format(tag,\n",
    "            #        epoch, batch_idx+1, ave_loss))\n",
    "\n",
    "        # testing\n",
    "        correct_cnt, ave_loss = 0, 0\n",
    "        total_cnt = 0\n",
    "        for batch_idx, (x, target) in enumerate(test_loader):\n",
    "            x = x.view(batch_size,-1)\n",
    "            out = F.softmax(model(x),dim=1)\n",
    "            loss = criterion(out, target)\n",
    "            _, pred_label = torch.max(out.data, 1)\n",
    "            total_cnt += x.data.size()[0]\n",
    "            correct_cnt += int((pred_label == target.data).sum())\n",
    "            # smooth average\n",
    "            ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "\n",
    "            if(batch_idx+1) == len(test_loader):\n",
    "                print('==>>> {}: epoch {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(tag,\n",
    "                    epoch, batch_idx+1, ave_loss, correct_cnt * 1.0 / float(total_cnt)))\n",
    "\n",
    "    gnn.update_graph()\n",
    "    for i in range(10):\n",
    "        loader = iter(train_loader)\n",
    "        samples = 0\n",
    "        X, y = [], []\n",
    "        while(samples < 5000):\n",
    "            X_t, y_t = loader.next()\n",
    "            X.append(X_t.view(batch_size,-1))\n",
    "            y.append(y_t)\n",
    "            samples += batch_size\n",
    "        samples = 0\n",
    "        gnn.add_cossim_hidden(X,y,out=i)\n",
    "        gnn2.add_random_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in zero_ins:\n",
    "    if len(z) == 2:\n",
    "        print(z[0],z[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in one_ins:\n",
    "    if len(o) == 2:\n",
    "        print(o[0],o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
